import sys
import time
import os
import cv2
import mediapipe as mp
from PyQt6.QtWidgets import QApplication
from PyQt6.QtCore import QThread, pyqtSignal, QTimer, Qt
from PyQt6.QtGui import QImage

# åŒ¯å…¥å°ˆæ¡ˆæ¨¡çµ„
from ui.main import MainUI
from ai.models import PoseEngine, VTuberRenderer
from core.state import SystemState
from core.gesture_engine import GestureEngine
from ai.llm_engine import OllamaCoach  # ç¢ºä¿å°å…¥ Ollama å¼•æ“
import json
def resource_path(relative_path):
    """ å–å¾—è³‡æºçµ•å°è·¯å¾‘ï¼Œå…¼å®¹é–‹ç™¼ç’°å¢ƒèˆ‡æ‰“åŒ…å¾Œçš„ç’°å¢ƒ """
    if hasattr(sys, '_MEIPASS'):
        # PyInstaller é‹è¡Œæ™‚çš„è‡¨æ™‚è·¯å¾‘
        return os.path.join(sys._MEIPASS, relative_path)
    # é–‹ç™¼ç’°å¢ƒä¸‹çš„ç•¶å‰è·¯å¾‘
    return os.path.join(os.path.abspath("."), relative_path)
# 1. è®€å– Llama é…ç½®
config_path = resource_path("llama_config.json")
with open(config_path, "r", encoding="utf-8") as f:
    config = json.load(f)

# 2. è¼‰å…¥ XGBoost æ¨¡å‹
model_path = resource_path("yoga_pose_model_RightFoot.json")
# å‡è¨­æ‚¨ä½¿ç”¨ xgboost çš„ load_model æˆ–è‡ªå®šç¾©è¼‰å…¥é‚è¼¯
# bst.load_model(model_path) 

# 3. è®€å–æ¨™ç±¤æª”
labels_path = resource_path("rightfoot.json")
with open(labels_path, "r", encoding="utf-8") as f:
    labels = json.load(f)

# å¼·åˆ¶è¼¸å‡ºå•Ÿå‹•è¨Šè™Ÿ
print("[System] æ­£åœ¨å•Ÿå‹•ç¨‹åº...", flush=True)

# --- 1. LLM éåŒæ­¥è™•ç†åŸ·è¡Œç·’ ---
class LLMWorker(QThread):
    finished = pyqtSignal(str)
    def __init__(self, coach, status):
        super().__init__()
        self.coach = coach
        self.status = status
        print(f"[LLM Worker] æº–å‚™è«‹æ±‚ AI å»ºè­°: {self.status}", flush=True)

    def run(self):
        # åŸ·è¡Œè€—æ™‚çš„ Ollama è«‹æ±‚
        res = self.coach.ask(self.status)
        self.finished.emit(res)

# --- 2. å½±åƒè™•ç†æ ¸å¿ƒåŸ·è¡Œç·’ ---
class VideoThread(QThread):
    raw_ready = pyqtSignal(QImage)
    vt_ready = pyqtSignal(QImage)
    # ç‹€æ…‹è¨Šè™Ÿå°æ‡‰ V14 UI: (is_active, fps, feedback, hand_x, hand_y)
    status_update = pyqtSignal(bool, float, str, float, float)
    gesture_cmd = pyqtSignal(str)

    def __init__(self, state):
        super().__init__()
        self.state = state
        print("[VideoThread] æ­£åœ¨åˆå§‹åŒ– AI è¦–è¦ºå¼•æ“...", flush=True)
        self.pose = PoseEngine()
        self.vt = VTuberRenderer()
        self.gesture_engine = GestureEngine()
        
        # åˆå§‹åŒ– MediaPipe Hands (åƒ…æ“æ§æ¨¡å¼ä½¿ç”¨)
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )

    def run(self):
        print("[VideoThread] æ­£åœ¨é–‹å•Ÿæ”å½±æ©Ÿ...", flush=True)
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("[VideoThread] âŒ éŒ¯èª¤ï¼šç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ", flush=True)
            return

        last_time = time.time()
        while not self.state.stop_signal:
            ret, frame = cap.read()
            if not ret: continue
            
            frame = cv2.flip(frame, 1)
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            anno_frame = frame.copy()
            skeleton = None
            feedback = ""
            is_active = False
            hand_x, hand_y = -1.0, -1.0

            # --- åˆ‡æ›é‚è¼¯ï¼šé‹å‹•ç”¨ Pose / æ“æ§ç”¨ Hands ---
            if self.state.mode == "EXERCISE":
                # åœ¨é‹å‹•æ¨¡å¼ä¸‹å‘¼å« PoseEngine çš„ process
                anno_frame, skeleton, feedback = self.pose.process(frame)
                if skeleton:
                    # é‹å‹•æ¨¡å¼ä¸‹ä½¿ç”¨ Pose å³é£ŸæŒ‡ (20) ä½œç‚ºè§¸ç™¼é»
                    r_idx = skeleton.landmark[20]
                    if r_idx.visibility > 0.5:
                        hand_x, hand_y = r_idx.x, r_idx.y
            else:
                # --- ä¿®æ­£è™•ï¼šæ­£ç¢ºè™•ç†æ‰‹å‹¢åµæ¸¬é‚è¼¯ ---
                # æ“æ§æ¨¡å¼ä¸‹ä½¿ç”¨ MediaPipe Hands ä¸¦å°æ¥ GestureEngine æ–¹æ³•
                results = self.hands.process(rgb_frame)
                feedback = "æ‰‹éƒ¨æ“æ§æ¨¡å¼"
                
                if results.multi_hand_landmarks:
                    lm = results.multi_hand_landmarks[0]
                    # å‘¼å« GestureEngine å®šç¾©çš„æ–¹æ³•
                    is_active = self.gesture_engine.is_fist(lm)
                    cmd = self.gesture_engine.get_swipe_command(
                        lm, is_active, self.state.current_page
                    )
                    
                    # å–å¾—é£ŸæŒ‡æŒ‡å°–ä½ç½® (8 è™Ÿé») ç”¨æ–¼ M éµæ‡¸åœ
                    hand_x, hand_y = lm.landmark[8].x, lm.landmark[8].y
                    
                    if cmd:
                        self.gesture_cmd.emit(cmd)

            # è½‰æ› QImage
            def to_qimg(img):
                h, w, c = img.shape
                return QImage(img.data, w, h, c*w, QImage.Format.Format_BGR888).copy()

            self.raw_ready.emit(to_qimg(anno_frame))
            self.vt_ready.emit(to_qimg(self.vt.render(skeleton)))
            
            curr_time = time.time()
            fps = 1.0 / (curr_time - last_time) if (curr_time - last_time) > 0 else 0
            last_time = curr_time
            
            self.status_update.emit(is_active, fps, feedback, hand_x, hand_y)
            
        cap.release()
        self.hands.close()

# --- 3. ä¸»ç¨‹åºå…¥å£ ---
def main():
    print("[Main] é€²å…¥ä¸»å‡½å¼", flush=True)
    app = QApplication(sys.argv)
    
    state = SystemState()
    ui = MainUI(state)
    
    # ğŸ’¡ é—œéµé»ï¼šåœ¨é€™è£¡å¼·åˆ¶åˆå§‹åŒ– OllamaCoach
    print("[Main] æ­£åœ¨å¼·åˆ¶åˆå§‹åŒ– Ollama æ•™ç·´...", flush=True)
    try:
        coach = OllamaCoach(model="llama3")
        print("[Main] âœ… OllamaCoach ç‰©ä»¶å»ºç«‹å®Œæˆ", flush=True)
    except Exception as e:
        print(f"[Main] âŒ OllamaCoach åˆå§‹åŒ–å¤±æ•—: {e}", flush=True)
        coach = None

    video = VideoThread(state)
    video.raw_ready.connect(ui.update_video)
    video.vt_ready.connect(ui.update_vtuber)
    video.status_update.connect(ui.update_status)
    video.gesture_cmd.connect(ui.handle_command)

    # --- 4. æ™ºæ…§æ•™ç·´è§¸ç™¼é‚è¼¯ ---
    last_coach_time = 0
    last_status = ""

    def handle_coach_trigger(is_active, fps, feedback, x, y):
        nonlocal last_coach_time, last_status
        if state.mode != "EXERCISE" or coach is None: return
        
        current_time = time.time()
        # è§¸ç™¼æ¢ä»¶ï¼š15ç§’å†·å»ä¸”ç‹€æ…‹æ–‡å­—æœ‰è®Š (ä¾‹å¦‚: åç§» -> æ­£ç¢º)
        if current_time - last_coach_time > 15 and feedback != last_status:
            if "æ­£ç¢º" in feedback or "åç§»" in feedback:
                execute_llm_request(feedback)

    def execute_llm_request(status_text):
        """åŸ·è¡Œ LLM è«‹æ±‚çš„å…±ç”¨å‡½å¼"""
        nonlocal last_coach_time, last_status
        print(f"[Coach] è§¸ç™¼ AI å»ºè­°è«‹æ±‚: {status_text}", flush=True)
        last_coach_time = time.time()
        last_status = status_text
        
        worker = LLMWorker(coach, status_text)
        if hasattr(ui, 'show_coach'):
            worker.finished.connect(ui.show_coach)
        worker.start()
        ui._llm_worker = worker

    # --- 5. æ‰‹å‹•æ¸¬è©¦é‚è¼¯ (æŒ‰ä¸‹éµç›¤ T éµè§¸ç™¼) ---
    def manual_test_trigger(event):
        if event.key() == Qt.Key.Key_T:
            print("[Test] æª¢æ¸¬åˆ°æŒ‰ä¸‹ T éµï¼Œæ­£åœ¨æ‰‹å‹•è§¸ç™¼ LLM æ¸¬è©¦...", flush=True)
            execute_llm_request("æ­£ç¢ºå³å¹³è¡¡ (æ‰‹å‹•æ¸¬è©¦)")
        # å‘¼å«åŸæœ¬çš„ keyPressEvent (å¦‚æœæœ‰)
        MainUI.keyPressEvent(ui, event)

    # å°‡æ¸¬è©¦å‡½å¼æ³¨å…¥ UI å¯¦ä¾‹
    ui.keyPressEvent = manual_test_trigger

    video.status_update.connect(handle_coach_trigger)

    video.start()
    print("[Main] æ­£åœ¨é¡¯ç¤ºä¸»è¦–çª—...", flush=True)
    print("[Main] ğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥åœ¨è¦–çª—å•Ÿå‹•å¾ŒæŒ‰ä¸‹éµç›¤ã€Tã€éµä¾†æ‰‹å‹•æ¸¬è©¦ LLM è¼¸å‡ºã€‚", flush=True)
    ui.show()
    sys.exit(app.exec())

if __name__ == "__main__":
    main()